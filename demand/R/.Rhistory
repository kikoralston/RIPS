df.ms$subregion <- tolower(df.ms$subregion)
df.ms$subregion <- trimws(df.ms$subregion)
county.maps.2 <- combine.types(county.maps.2, df.ms)
# Entergy ARkansas ----
# 'http://www.entergy-arkansas.com/about_entergy/counties.aspx'
ff <- file(description = paste0(path.data,'Arkansas.md'))
df.ar <- as.data.frame(readLines(con=ff))
close(ff)
names(df.ar) <- c('subregion')
df.ar$region <- "arkansas"
df.ar$type <- 'Entergy'
df.ar$subregion <- tolower(df.ar$subregion)
df.ar$subregion <- trimws(df.ar$subregion)
df.ar$subregion <- gsub('\\.', "", df.ar$subregion, ignore.case = FALSE)
county.maps.2 <- combine.types(county.maps.2, df.ar)
# Entergy MS ----
ff <- file(description = paste0(path.data,'Entergy MS.md'))
df.ms2 <- as.data.frame(readLines(con=ff))
close(ff)
names(df.ms2) <- c('subregion')
df.ms2$region <- "mississippi"
df.ms2$type <- 'Entergy'
df.ms2$subregion <- tolower(df.ms2$subregion)
df.ms2$subregion <- trimws(df.ms2$subregion)
df.ms2$subregion <- gsub('\\.', "", df.ms2$subregion, ignore.case = FALSE)
county.maps.2 <- combine.types(county.maps.2, df.ms2)
# Alabama Power ----
# assign all remaining empty counties in alabama to alabama power
idx.rows <- which(county.maps.2$region == "alabama" & is.na(county.maps.2$type))
county.maps.2[idx.rows, c("type")] <- 'AL Power'
# Carolinas ----
# South Carolina Electric & Gas Company
# https://www.sceg.com/about-us/our-community/service-area-map
#ff <- file(description = paste0(path.data,'SCEG.md'))
#df.sceg <- readLines(con=ff)
#close(ff)
#df.sceg <- ldply(.data=df.sceg, .fun=parseLines, type='SCEG')
#county.maps.2 <- combine.types(county.maps.2, df.sceg)
# Santee Cooper
# https://www.santeecooper.com/about-santee-cooper/index.aspx
# all counties
t.sc <- get.df.counties.state('South Carolina')
t.sc$region <- 'south carolina'
t.sc$type <- 'santee+sceg+Duke'
county.maps.2 <- combine.types(county.maps.2, t.sc)
t.nc <- get.df.counties.state('North Carolina')
t.nc$region <- 'north carolina'
t.nc.aux <- df.tva %>% filter(region == 'north carolina') %>% select(region, subregion)
t.nc <- t.nc %>% setdiff(t.nc.aux)
t.nc$type <- 'santee+sceg+Duke'
county.maps.2 <- combine.types(county.maps.2, t.nc)
county.maps.2$type <- as.factor(county.maps.2$type)
pal <- c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c',
'#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928')
library(plyr)
library(dplyr)
library(maps)
library(ggplot2)
county_map <- map_data("county")
state_map <- map_data("state")
setwd('/Users/kiko/Google Drive/CMU/RIPS/R/')
path.data <- '/Users/kiko/Google Drive/CMU/RIPS/TVA info/'
path.out <- '~/Google Drive/CMU/RIPS/SERC companies/'
source('CENSUS_DATA.R')
setwd('/Users/kiko/Google Drive/CMU/RIPS/git/demand/')
path.data <- '/Users/kiko/Google Drive/CMU/RIPS/TVA info/'
path.out <- '~/Google Drive/CMU/RIPS/SERC companies/'
source('CENSUS_DATA.R')
setwd('/Users/kiko/Google Drive/CMU/RIPS/git/demand/R/')
path.data <- '/Users/kiko/Google Drive/CMU/RIPS/TVA info/'
path.out <- '~/Google Drive/CMU/RIPS/SERC companies/'
source('CENSUS_DATA.R')
ff <- file(description = paste0(path.data, 'TVA counties.md'))
tva.counties <- readLines(con=ff)
close(ff)
parseLines <- function(x, type) {
require(plyr)
require(dplyr)
require(maps)
# ignorelines with '#' (assume that are commentaries)
if (substr(x,1,1) !='#') {
# separate into state and list of counties
y <- unlist(strsplit(x, split = ";"))
# substitute word 'and' to a comma
y[2] <- gsub(' and ', ',', y[2], ignore.case = TRUE)
# convert states's two letter code to name
t <- read.table(file=url('https://www2.census.gov/geo/docs/reference/state.txt'),
header = TRUE, sep = '|', stringsAsFactors = FALSE)
state.name <- tolower(t$STATE_NAME[t$STUSAB == y[1]])
if(trimws(tolower(y[2])) == "all") {
states_map <- map_data("county")
z <- states_map %>% filter(region == state.name) %>% select(subregion) %>% unlist(use.names = FALSE) %>% unique()
} else {
z <- tolower(trimws(unlist(strsplit(y[2], split = ","))))
}
return(data.frame(region=state.name, subregion=z, type=type,
stringsAsFactors=FALSE))
} else {
return(NULL)
}
}
df.tva <- ldply(.data=tva.counties, .fun=parseLines, type='TVA')
county.maps.2 <- left_join(county_map, df.tva, by = c('region', 'subregion'))
library(RCurl)
library(XML)
get.list.counties <- function(x){
# remove special characters
#xx <- trimws(gsub("\r|\n|\t"," ", x, ignore.case = TRUE))
xx <- trimws(x)
xx <- gsub(rawToChar(as.raw(strtoi(c("0xc2","0xa0")))),
"", xx, ignore.case = TRUE)
# get list of counties
list.counties <- trimws(substring(xx, regexpr("Counties served:",
xx, ignore.case = TRUE)+16))
# remove words "Web Site" in the end
list.counties <- gsub("Web Site", "", list.counties, ignore.case = TRUE)
# remove cases with 'http:'
if (regexpr('http:', list.counties, ignore.case = TRUE) > 0) {
list.counties <- substring(list.counties, 1, regexpr('http:', list.counties, ignore.case = TRUE)-1)
}
# first word in the text should be the name of the company
# remove that too
if (regexpr('EMC', xx, ignore.case = TRUE) > 0) {
name.company <- substring(xx, 1, regexpr('EMC', xx, ignore.case = TRUE)+3-1)
list.counties <- gsub(name.company, "", list.counties, ignore.case = TRUE)
} else {
# if 'EMC' is not found, looks for 'P.O.'
name.company <- substring(xx, 1, regexpr('P.O.', xx, ignore.case = TRUE)-1)
list.counties <- gsub(name.company, "", list.counties, ignore.case = TRUE)
}
# change word 'and' to a comma
list.counties <- gsub(" and ", ",", list.counties, ignore.case = TRUE)
# change word ' in ' to a comma
list.counties <- gsub(" in ", ",", list.counties, ignore.case = TRUE)
# change word ';' to a comma
list.counties <- gsub(";", ",", list.counties, ignore.case = TRUE)
# change word 'parts of' to a comma
list.counties <- gsub("parts of", "", list.counties, ignore.case = TRUE)
# split string to a vector and remove white spaces
list.counties <- unlist(strsplit(list.counties, split=","))
# fix some special cases where the name of the company remains in the
# last element of list
n.counties <- length(list.counties)
list.counties[[n.counties]] <- gsub('EMC|REMC', "",
list.counties[[n.counties]], ignore.case = TRUE)
first.word <- strsplit(name.company, split=" ")[[1]][1]
list.counties[[n.counties]] <- gsub(first.word, "", list.counties[[n.counties]],
ignore.case = TRUE)
list.counties <- sapply(X=list.counties, FUN=trimws,  USE.NAMES=FALSE)
return(list.counties)
}
combine.types <- function(df1, df2) {
df.out <- left_join(df1, df2, by = c('region', 'subregion'))
type <- rep(NA, nrow(df.out))
type[!is.na(df.out$type.x)] <- df.out$type.x[!is.na(df.out$type.x)]
type[!is.na(df.out$type.y)] <- df.out$type.y[!is.na(df.out$type.y)]
df.out$type.x <- NULL
df.out$type.y <- NULL
df.out$type <- type
return(df.out)
}
t.ga <- get.df.counties.state('Georgia')
t.ga$region <- 'georgia'
t.ga$type <- 'OPC+GP+MEAG'
t.ga$subregion[t.ga$subregion == 'dekalb'] <- 'de kalb'
county.maps.2 <- combine.types(county.maps.2, t.ga)
ff <- file(description = paste0(path.data,'Old Dominion.md'))
old.dominion.raw <- readLines(con=ff)
close(ff)
old.dominion <- ldply(.data=old.dominion.raw, .fun=parseLines, type='old dominion')
county.maps.2 <- combine.types(county.maps.2, old.dominion)
html <- getURL('http://www.sipower.org/p/members.php')
doc <- htmlParse(html)
# get XML nodes marked as tbody (tables) and convert to R list
ww <- xpathApply(doc, path = "//p[@class=\"internal\"]", xmlValue)
ww[[1]] <- NULL
ww[[8]] <- NULL
get.list.counties.illinois <- function(x){
# remove special characters
xx <- trimws(x)
xx <- gsub(rawToChar(as.raw(strtoi(c("0xc2","0xa0")))),
"", xx, ignore.case = TRUE)
# get list of counties
list.counties <- trimws(substring(xx, regexpr("Counties served:",
xx, ignore.case = TRUE)+16))
# remove words "Web Site" in the end
list.counties <- gsub("Web Site", "", list.counties, ignore.case = TRUE)
# remove cases with 'http:' in the end
if (regexpr('http:', list.counties, ignore.case = TRUE) > 0) {
list.counties <- substring(list.counties, 1, regexpr('http:', list.counties, ignore.case = TRUE)-1)
}
# remove cases with 'www.' in the end
if (regexpr('www.', list.counties, ignore.case = TRUE) > 0) {
list.counties <- substring(list.counties, 1, regexpr('www.', list.counties, ignore.case = TRUE)-1)
}
# remove '.'
list.counties <- gsub("\\.", "", list.counties, ignore.case = TRUE)
# change word 'and' to a comma
list.counties <- gsub(" and ", ",", list.counties, ignore.case = TRUE)
# split string to a vector and remove white spaces
list.counties <- unlist(strsplit(list.counties, split=","))
list.counties <- trimws(list.counties)
return(list.counties)
}
list.counties.IL <- sapply(X=ww, FUN=get.list.counties.illinois)
df.IL <- ldply(.data=list.counties.IL, .fun=as.data.frame,
stringAsFactors=FALSE)
names(df.IL) <- 'subregion'
df.IL$subregion <- as.character(df.IL$subregion)
df.IL$region <- "illinois"
df.IL$type <- 'SIPC'
df.IL$subregion <- tolower(df.IL$subregion)
county.maps.2 <- combine.types(county.maps.2, df.IL)
t.mo <- get.df.counties.state('Missouri')
t.mo$region <- 'missouri'
t.mo$type <- 'aeci'
ff <- file(description = paste0(path.data,'aeci.md'))
aeci <- readLines(con=ff)
close(ff)
aeci <- ldply(.data=aeci, .fun=parseLines, type='aeci')
aeci <- rbind(t.mo, aeci)
county.maps.2 <- combine.types(county.maps.2, aeci)
# EKPC ----
ff <- file(description = paste0(path.data,'ekpc.md'))
ekpc <- readLines(con=ff)
close(ff)
ekpc <- ldply(.data=ekpc, .fun=parseLines, type='ekpc')
county.maps.2 <- combine.types(county.maps.2, ekpc)
# Power South ----
ff <- file(description = paste0(path.data,'Power South.md'))
ps <- readLines(con=ff)
close(ff)
ps <- ldply(.data=ps, .fun=parseLines, type='power south')
county.maps.2 <- combine.types(county.maps.2, ps)
# Gulf Power ----
# according to gulf power website, the counties in florida
# served by power south are also served by Gulf Power
gulf <- ps %>% filter(region == 'florida') %>% distinct()
gulf$type <- 'Gulf'
county.maps.2 <- combine.types(county.maps.2, gulf)
ff <- file(description = paste0(path.data,'Mississippi.md'))
df.ms <- as.data.frame(readLines(con=ff))
close(ff)
names(df.ms) <- c('subregion')
df.ms$region <- "mississippi"
df.ms$type <- 'MS Power'
df.ms$subregion <- tolower(df.ms$subregion)
df.ms$subregion <- trimws(df.ms$subregion)
county.maps.2 <- combine.types(county.maps.2, df.ms)
ff <- file(description = paste0(path.data,'Arkansas.md'))
df.ar <- as.data.frame(readLines(con=ff))
close(ff)
names(df.ar) <- c('subregion')
df.ar$region <- "arkansas"
df.ar$type <- 'Entergy'
df.ar$subregion <- tolower(df.ar$subregion)
df.ar$subregion <- trimws(df.ar$subregion)
df.ar$subregion <- gsub('\\.', "", df.ar$subregion, ignore.case = FALSE)
county.maps.2 <- combine.types(county.maps.2, df.ar)
ff <- file(description = paste0(path.data,'Entergy MS.md'))
df.ms2 <- as.data.frame(readLines(con=ff))
close(ff)
names(df.ms2) <- c('subregion')
df.ms2$region <- "mississippi"
df.ms2$type <- 'Entergy'
df.ms2$subregion <- tolower(df.ms2$subregion)
df.ms2$subregion <- trimws(df.ms2$subregion)
df.ms2$subregion <- gsub('\\.', "", df.ms2$subregion, ignore.case = FALSE)
county.maps.2 <- combine.types(county.maps.2, df.ms2)
idx.rows <- which(county.maps.2$region == "alabama" & is.na(county.maps.2$type))
county.maps.2[idx.rows, c("type")] <- 'AL Power'
t.sc <- get.df.counties.state('South Carolina')
t.sc$region <- 'south carolina'
t.sc$type <- 'santee+sceg+Duke'
county.maps.2 <- combine.types(county.maps.2, t.sc)
t.nc <- get.df.counties.state('North Carolina')
t.nc$region <- 'north carolina'
t.nc.aux <- df.tva %>% filter(region == 'north carolina') %>% select(region, subregion)
t.nc <- t.nc %>% setdiff(t.nc.aux)
t.nc$type <- 'santee+sceg+Duke'
county.maps.2 <- combine.types(county.maps.2, t.nc)
county.maps.2$type <- as.factor(county.maps.2$type)
pal <- c('#a6cee3','#1f78b4','#b2df8a','#33a02c','#fb9a99','#e31a1c',
'#fdbf6f','#ff7f00','#cab2d6','#6a3d9a','#ffff99','#b15928')
g <- ggplot() +
geom_polygon(data=county.maps.2, aes(x=long, y=lat, group=group), colour='gray80') +
geom_polygon(data=county.maps.2, aes(x=long, y=lat, group=group, fill=type)) +
geom_path(data=state_map, aes(x=long, y=lat, group=group)) +
theme_bw() + coord_map("polyconic", xlim=c(-100, -75),
ylim=c(30, 40)) +
#  scale_fill_discrete(na.value = 'white')
scale_fill_manual(values=pal, na.value="white")
g
png(filename = paste0(path.out, 'serc_map.png'), width = 1600, height = 1000,
res = 300)
print(g)
dev.off()
png(filename = paste0(path.out, 'serc_map.png'), width = 2000, height = 2000,
res = 300)
print(g)
dev.off()
? png
library('plyr')
library('dplyr')
library(lubridate)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(grid)
companies <- read.csv(file = paste('/Users/kiko/GoogleDrive/CMU/RIPS/SERC companies/SERC.csv'),
stringsAsFactors = FALSE)
companies[companies$Name.ferc != "", c("Name")] <- companies[companies$Name.ferc != "", c("Name.ferc")]
companies <- unlist(companies %>% select(Name, FERC) %>% filter(FERC == 1) %>% select(Name))
source('readFERC.R')
ferc.zip <- downloadFERCdata()
df.results <- data.frame(name=as.character(),
year.ini=as.numeric(),
year.end=as.numeric(),
n.points=as.numeric(),
mean.load=as.numeric())
pb <- txtProgressBar(min = 0, max = length(companies), style = 3)
setTxtProgressBar(pb, 0)
data.ferc <- NULL
for (i in 1:length(companies)) {
aux <- readFercNew(get.online = FALSE,
ferc.zip = ferc.zip,
name.util = companies[i])
aux$name <- companies[i]
if(i==1){
data.ferc <- aux
} else{
data.ferc <- rbind(data.ferc, aux)
}
setTxtProgressBar(pb, i)
}
df.summary <- data.ferc %>% group_by(name) %>%
summarize(year.ini=min(year(time)), year.end=max(year(time)),
n.points=n(), mean.load=mean(load, na.rm = TRUE))
row.names(df.summary) <- NULL
table.markdown(df = df.summary)
xx <- data.ferc %>% group_by(time) %>%
summarise(hourly.load=sum(load, na.rm=TRUE)) %>%
mutate(year=year(time))
annual.summary <- data.ferc %>% mutate(year=year(time)) %>% group_by(year) %>%
summarize(annual.energy=sum(load, na.rm=TRUE)/1e3,
n.companies=length(unique(name)))
zz <- left_join(xx, annual.summary, by='year')
# Read EIA data ----
source("excel.R")
cat('\n\nReading EIA data ...\n')
eia.url <- 'https://www.eia.gov/electricity/data/eia411/xls/net_energy_load_2015.xls'
eia.file <- tempfile()
download.file(url = eia.url, destfile = eia.file, method='curl')
f.csv <- tempfile(fileext = ".csv")
f.xls <- eia.file
convert.xls2csv(xls.file = f.xls, csv.file = f.csv)
list.csv <- list.files(path=tempdir(), pattern = ".csv",
full.names = TRUE)
f.csv <- list.csv[1]
eia.data <- read.table(file=f.csv, sep = ",",
skip=8, nrows=20, header = TRUE,
stringsAsFactors = FALSE)
eia.data <- eia.data[ ,c(-1)]
row.serc <- grep("SERC", eia.data[,1], ignore.case = TRUE)
eia.aux <- eia.data[row.serc, ]
eia.aux <- eia.aux[-1]
eia.aux <- data.frame(year=seq(1990, 2026), energy=unlist(eia.aux))
a <- as.POSIXct(paste0(eia.aux$year,"-01-01"), format="%Y-%m-%d")
b <- as.POSIXct(paste0(eia.aux$year,"-12-31"), format="%Y-%m-%d")
eia.data.serc <- rbind(data.frame(time=a, energy=eia.aux$energy),
data.frame(time=b, energy=eia.aux$energy))
eia.data.serc <- eia.data.serc[order(eia.data.serc$time), ]
eia.data.serc <- eia.data.serc %>% filter(year(time) %in% seq(2006, 2015))
eia.data.serc <- eia.data.serc %>% left_join(zz, by='time') %>%
select(time, energy, annual.energy)
names(eia.data.serc) <- c('time', 'EIA', 'FERC.Data')
eia.data.serc <- eia.data.serc %>% gather(key='type', value='energy', EIA:FERC.Data)
g1 <- ggplot(data=zz) + geom_line(aes(x=time, y=hourly.load/1e3)) +
ylab("SERC Hourly Load (GW)") +
xlab("")
g2 <- ggplot() +
geom_line(data=eia.data.serc, aes(x=time, y=energy/1e3, linetype=type)) +
ylab(expression(paste("SERC Annual Consumption(", 10^3, " MWh)"))) +
xlab("") + theme(legend.position=c(1,1), legend.justification=c(1,1))
g3 <- ggplot(data=zz) + geom_line(aes(x=time, y=n.companies)) +
ylab("# companies in data set") +
xlab("time") +
theme(axis.title.y=element_text(margin=margin(0,20,0,0)))
g1 <- ggplot_gtable(ggplot_build(g1))
g2 <- ggplot_gtable(ggplot_build(g2))
g3 <- ggplot_gtable(ggplot_build(g3))
maxWidth <- unit.pmax(g1$widths[2:3], g2$widths[2:3], g3$widths[2:3])
g1$widths[2:3] <- maxWidth
g2$widths[2:3] <- maxWidth
g3$widths[2:3] <- maxWidth
library('plyr')
library('dplyr')
library(lubridate)
library(tidyr)
library(ggplot2)
library(gridExtra)
library(grid)
companies <- read.csv(file = paste('/Users/kiko/Google Drive/CMU/RIPS/SERC companies/SERC.csv'),
stringsAsFactors = FALSE)
companies[companies$Name.ferc != "", c("Name")] <- companies[companies$Name.ferc != "", c("Name.ferc")]
companies <- unlist(companies %>% select(Name, FERC) %>% filter(FERC == 1) %>% select(Name))
source('readFERC.R')
ferc.zip <- downloadFERCdata()
ferc.zip <- downloadFERCdata(ferc.zip = 'form714-database.zip')
df.results <- data.frame(name=as.character(),
year.ini=as.numeric(),
year.end=as.numeric(),
n.points=as.numeric(),
mean.load=as.numeric())
pb <- txtProgressBar(min = 0, max = length(companies), style = 3)
setTxtProgressBar(pb, 0)
data.ferc <- NULL
for (i in 1:length(companies)) {
aux <- readFercNew(get.online = FALSE,
ferc.zip = ferc.zip,
name.util = companies[i])
aux$name <- companies[i]
if(i==1){
data.ferc <- aux
} else{
data.ferc <- rbind(data.ferc, aux)
}
setTxtProgressBar(pb, i)
}
df.summary <- data.ferc %>% group_by(name) %>%
summarize(year.ini=min(year(time)), year.end=max(year(time)),
n.points=n(), mean.load=mean(load, na.rm = TRUE))
row.names(df.summary) <- NULL
table.markdown(df = df.summary)
xx <- data.ferc %>% group_by(time) %>%
summarise(hourly.load=sum(load, na.rm=TRUE)) %>%
mutate(year=year(time))
annual.summary <- data.ferc %>% mutate(year=year(time)) %>% group_by(year) %>%
summarize(annual.energy=sum(load, na.rm=TRUE)/1e3,
n.companies=length(unique(name)))
zz <- left_join(xx, annual.summary, by='year')
table.markdown <- function(df, name.file=''){
# writes data frame in markdown table format to a txt file
df$mean.load <- round(df$mean.load, 0)
df$name <- as.character(df$name)
nc <- ncol(df)
cat('| ', paste(names(df), collapse =" | "), ' |\n',
'|', rep('----- |', nc), '\n', sep='', file = name.file)
for (i in 1:nrow(df)) {
cat('|', paste(df[i, ], collapse =" | "), '| \n', file = name.file)
}
}
source("excel.R")
cat('\n\nReading EIA data ...\n')
eia.url <- 'https://www.eia.gov/electricity/data/eia411/archive/net_energy_load_2015.xls'
eia.file <- tempfile()
download.file(url = eia.url, destfile = eia.file, method='curl')
f.csv <- tempfile(fileext = ".csv")
f.xls <- eia.file
convert.xls2csv(xls.file = f.xls, csv.file = f.csv)
list.csv <- list.files(path=tempdir(), pattern = ".csv",
full.names = TRUE)
f.csv <- list.csv[1]
eia.data <- read.table(file=f.csv, sep = ",",
skip=8, nrows=20, header = TRUE,
stringsAsFactors = FALSE)
eia.data <- eia.data[ ,c(-1)]
row.serc <- grep("SERC", eia.data[,1], ignore.case = TRUE)
eia.aux <- eia.data[row.serc, ]
eia.aux <- eia.aux[-1]
eia.aux <- data.frame(year=seq(1990, 2026), energy=unlist(eia.aux))
a <- as.POSIXct(paste0(eia.aux$year,"-01-01"), format="%Y-%m-%d")
b <- as.POSIXct(paste0(eia.aux$year,"-12-31"), format="%Y-%m-%d")
eia.data.serc <- rbind(data.frame(time=a, energy=eia.aux$energy),
data.frame(time=b, energy=eia.aux$energy))
eia.data.serc <- eia.data.serc[order(eia.data.serc$time), ]
eia.data.serc <- eia.data.serc %>% filter(year(time) %in% seq(2006, 2015))
eia.data.serc <- eia.data.serc %>% left_join(zz, by='time') %>%
select(time, energy, annual.energy)
names(eia.data.serc) <- c('time', 'EIA', 'FERC.Data')
eia.data.serc <- eia.data.serc %>% gather(key='type', value='energy', EIA:FERC.Data)
g1 <- ggplot(data=zz) + geom_line(aes(x=time, y=hourly.load/1e3)) +
ylab("SERC Hourly Load (GW)") +
xlab("")
g2 <- ggplot() +
geom_line(data=eia.data.serc, aes(x=time, y=energy/1e3, linetype=type)) +
ylab(expression(paste("SERC Annual Consumption(", 10^3, " MWh)"))) +
xlab("") + theme(legend.position=c(1,1), legend.justification=c(1,1))
g3 <- ggplot(data=zz) + geom_line(aes(x=time, y=n.companies)) +
ylab("# companies in data set") +
xlab("time") +
theme(axis.title.y=element_text(margin=margin(0,20,0,0)))
g1 <- ggplot_gtable(ggplot_build(g1))
g2 <- ggplot_gtable(ggplot_build(g2))
g3 <- ggplot_gtable(ggplot_build(g3))
maxWidth <- unit.pmax(g1$widths[2:3], g2$widths[2:3], g3$widths[2:3])
g1$widths[2:3] <- maxWidth
g2$widths[2:3] <- maxWidth
g3$widths[2:3] <- maxWidth
grid.arrange(g1, g2, g3)
g1
plot(g1)
plot(g2)
plot(g3)
path.plot
path.plot
path.plot <- '/Users/kiko/Google Drive/CMU/RIPS/meetings/201803/slides'
path.plot <- '/Users/kiko/Google Drive/CMU/RIPS/meetings/201803/slides/'
png(paste0(path.plot, 'serc_data.png'), width=11, height=8.5, units='in',
res=300 )
grid.arrange(g1, g2, g3)
dev.off()
