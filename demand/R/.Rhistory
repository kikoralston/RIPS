range(util1[!is.infinite(util1)])
bins <- pretty(x=util1, n=nclass.FD(!is.infinite(util1)), min.n = 1)
bw2 <- bins[2] - bins[1]
bw2
bins
util1 <- utility.function(x=results.mc1, R=1)
util2 <- utility.function(x=results.mc2, R=1)
combined.df.2 <- data.frame(item.c=util1, item.d=util2)
combined.df.2 <- combined.df.2 %>% gather(key='type', value='util', item.c:item.d)
df.comparison <- combined.df %>% group_by(type) %>%
summarize(mean=mean(util), quant5 = quantile(util, 0.05),
quant95 = quantile(util, 0.95))
util1 <- utility.function(x=results.mc1, R=1)
util2 <- utility.function(x=results.mc2, R=1)
combined.df.2 <- data.frame(item.c=util1, item.d=util2)
combined.df.2 <- combined.df.2 %>% gather(key='type', value='util', item.c:item.d)
df.comparison <- combined.df %>% group_by(type) %>%
summarize(mean=mean(util), quant5 = quantile(util, 0.05),
quant95 = quantile(util, 0.95))
df.comparison.2 <- combined.df.2 %>% group_by(type) %>%
summarize(mean=mean(util), quant5 = quantile(util, 0.05),
quant95 = quantile(util, 0.95))
range(util1)
bins <- pretty(x=util1, n=nclass.FD(util1), min.n = 1)
n=nclass.FD(util1)
n
pretty(x=util1, n=nclass.FD(util1), min.n = 1)
g <- ggplot(data=combined.df.2) +
geom_histogram(aes(x=npv, y=..density..), fill='red', alpha=0.6) +
facet_wrap(~type, nrow=2) + theme_bw() + xlab('NPV ($ Billion)')
print(g)
g <- ggplot(data=combined.df.2) +
geom_histogram(aes(x=util, y=..density..), fill='red', alpha=0.6) +
facet_wrap(~type, nrow=2) + theme_bw() + xlab('NPV ($ Billion)')
print(g)
util1 <- utility.function(x=results.mc1, R=10)
util2 <- utility.function(x=results.mc2, R=10)
combined.df.2 <- data.frame(item.c=util1, item.d=util2)
combined.df.2 <- combined.df.2 %>% gather(key='type', value='util', item.c:item.d)
df.comparison.2 <- combined.df.2 %>% group_by(type) %>%
summarize(mean=mean(util), quant5 = quantile(util, 0.05),
quant95 = quantile(util, 0.95))
#bins <- pretty(x=util1, n=nclass.FD(util1), min.n = 1)
#bw2 <- bins[2] - bins[1]
g <- ggplot(data=combined.df.2) +
geom_histogram(aes(x=util, y=..density..), fill='red', alpha=0.6) +
facet_wrap(~type, nrow=2) + theme_bw() + xlab('NPV ($ Billion)')
print(g)
util1 <- utility.function(x=results.mc1, R=1)
util2 <- utility.function(x=results.mc2, R=1)
combined.df.2 <- data.frame(item.c=util1, item.d=util2)
combined.df.2 <- combined.df.2 %>% gather(key='type', value='util', item.c:item.d)
? geom_density
ggplot() + geom_density(data=combined.df.2, aes(x=util, fill=type)) +theme_bw()
CE1 <- inv.utility.function(mean(util1))
CE2 <- inv.utility.function(mean(util2))
CE_upgrade <- inv.utility.function(mean(-10))
CE1 <- inv.utility.function(mean(util1))
CE2 <- inv.utility.function(mean(util2))
xx <- results.mc1 - 10
mean(xx)
mean(results.mc1)
xx <- -10 + results.mc1
mean(xx)
xx <- -10 - results.mc1
mean(xx)
sqrt(var(xx))
hist(xx, breaks='FD')
sum(xx > 0)/length(xx)
plot(x,(inv.utility.function(utility.function(x), R=0)))
library(ggplot2)
library(plyr)
library(dplyr)
library(lubridate)
library(tidyr)
library(timeDate)
# letter page: 8.5 by 11.0 inches
# margins: 1 inch
# \textwidth = 6.5 in
txt.width <- 6.5 #inches
setwd('~/Google Drive/CMU/RIPS/git/demand/R/')
path.out <- '~/Google Drive/CMU/RIPS/paper/august17/images/'
path.data <- '~/Google Drive/CMU/RIPS/R/data/'
source('complete_pipeline.R')
library(ggplot2)
library(plyr)
library(dplyr)
library(lubridate)
library(tidyr)
library(timeDate)
# letter page: 8.5 by 11.0 inches
# margins: 1 inch
# \textwidth = 6.5 in
txt.width <- 6.5 #inches
setwd('~/Google Drive/CMU/RIPS/git/demand/R/')
path.out <- '~/Google Drive/CMU/RIPS/paper/august17/images/'
path.data <- '~/Google Drive/CMU/RIPS/R/data/'
source('complete_pipeline.R')
source('~/R/gdrive.R')
View(plot.temp.load)
# August 2017
# some plots for the paper
library(ggplot2)
library(plyr)
library(dplyr)
library(lubridate)
library(tidyr)
library(timeDate)
# letter page: 8.5 by 11.0 inches
# margins: 1 inch
# \textwidth = 6.5 in
txt.width <- 6.5 #inches
setwd('~/Google Drive/CMU/RIPS/git/demand/R/')
path.out <- '~/Google Drive/CMU/RIPS/paper/august17/images/'
path.data <- '~/Google Drive/CMU/RIPS/R/data/'
source('complete_pipeline.R')
source('~/R/gdrive.R')
source('readFERC.R')
source('excel.R')
path.uw <- '~/Documents/UW_data/RH_airT_19790101_20161231.Francisco/RH_airT_19790101_20161231.Nashville.csv'
df.station <- readRDS(file='~/Google Drive/CMU/RIPS/R/data/723270_13897_TN.rds')
df.gridded <- read.data.meteo.uw(path.uw)
df.combined <- left_join(df.station, df.gridded, by=c("time"="time"))
df.combined <- df.combined[order(df.combined$time), ]
e.grid.data <- readRDS(file=paste0(path.data, 'egrid_data.rds'))
needs.data <- readRDS(file=paste0(path.data, 'needs_data.rds'))
eia.data <- read.eia.923.data()
utility.data <- get.utility.data(e.grid.data = e.grid.data,
needs.data = needs.data,
eia.data = eia.data)
# add expensive dummy generator
row.aux <- utility.data %>% filter(total.cost == max(total.cost, na.rm=TRUE))
row.aux$PSTATABB <- NA
row.aux$ORISPL <- 9999
row.aux$PNAME <- 'AUX'
row.aux$CNTYNAME <- NA
row.aux$PLFUELCT <- 'AUX'
row.aux$NAMEPCAP <- 10000
row.aux$Cap.MW <- 10000
row.aux$FINAL.CAP <- 10000
row.aux$total.cost <- row.aux$total.cost*1.5
utility.data <- rbind(utility.data, row.aux)
remote.ferc <- TRUE
ferc.zip <- 'form714-database.zip'
df.ferc.final <- readFercNew(get.online = remote.ferc, ferc.zip = ferc.zip)
year.end <- max(year(df.ferc.final$time))
year.ini <- min(year(df.ferc.final$time))
df.total <- full_join(df.combined, df.ferc.final, by='time')
df.final <- df.total %>%
transmute(time=time, temp=temp.y, dp=dp.y, load=load) %>%
as.data.frame()
df.final <- df.final %>% arrange(time)
# there are some observations with demand = 0 which is not possible
# set them to NA (not observed)
df.final$load[df.final$load == 0] <- NA
# there are some observations where temperature is NA but dew.point
# has a numerical value. This can cause error in the interaction term
df.final$dp[is.na(df.final$temp)] <- NA
df.final$temp[is.na(df.final$dp)] <- NA
# create new columns for regression
# calendar variables
df.final <- compute.calendar.variables(df.final)
df.final <- cbind(data.frame(year=year(df.final$time)), df.final)
df.final <- df.final %>% filter(year >= year.ini & year <= year.end)
# ___ plot FERC data ----
a <- df.final %>% select(time, load) %>% mutate(load=ifelse(is.na(load),0,load)) %>%
mutate(mov.avg=stats::filter(load, rep(1/8760, 8760), sides = 1)) %>%
mutate(load=ifelse(load == 0,NA,load))
# add shaded areas for summer periods
summer.months <- df.final %>% group_by(year) %>%
filter(season == "Summer") %>%
summarise(date.ini=min(time, na.rm=TRUE),
date.end=max(time,na.rm=TRUE))
g <- ggplot(data=a) +
geom_rect(data=summer.months,
aes(xmin=date.ini, xmax=date.end),
ymin=-Inf, ymax=Inf, alpha=.45,
fill="gray") +
geom_line(aes(x=time, y=load/1e3), colour='darkgray') +
geom_line(aes(x=time, y=mov.avg/1e3), colour='black') +
theme_bw() + xlab('time') + ylab('Hourly Load (GW)')
fname <- paste0(path.out, 'fercdata.png')
plot.and.share(g=g, name.file = fname,
w=txt.width, h=9/16*txt.width)
df.final %>% group_by(year(time)) %>%  summarise(annual.avg=mean(load, na.rm=TRUE))
# ___ scatter plot temp vs load ----
g <- scatter.plot.temp.load(df.final)
fname <- paste0(path.out, 'plotTempLoad_1.png')
plot.and.share(g=g, name.file = fname,w=0.8*txt.width, h=0.8*txt.width)
# ___ fit regression model ----
temp.breaks <- c(-30, 0, 10, 20, 30)
lm.model <- fit.lm.model(df=df.final, temp.breaks=temp.breaks,
type.humidity = NULL)
if (FALSE) {
# compute Newey West standard errors
library(sandwich)
library(lmtest) # note this is needed for coeftest
se.ols <- coeftest(lm.model$model) # default OLS setting
df.se.results <- data.frame(names=as.character(), lag=as.numeric(),
std.error=as.numeric(),
t.value=as.numeric(),
p.value=as.numeric())
df.se.results <- rbind(df.se.results,
data.frame(names=names(se.ols[c(12:17),4]),
lag=0, t.value=se.ols[c(12:17),3],
p.value=se.ols[c(12:17),4]))
lags <- c(12, 24, 48, 72, 96, 120, 144, 168, 216, 264, 312, 336, 504)
for (l in lags) {
cat('---- Computing NW standard errors for lag = ', l, ' -----\n')
a <- coeftest(lm.model$model, vcov=NeweyWest(lm.model$model,lag=l,
prewhite=FALSE))
df.se.results <- rbind(df.se.results,
data.frame(names=names(a[c(12:17),4]),
lag=l, t.value=a[c(12:17),3],
p.value=a[c(12:17),4]))
h0.region <- qnorm(c(0.025, 0.975))
ggplot(data=df.se.results) +
geom_line(aes(x=lag, y=t.value, colour=names)) +
geom_point(aes(x=lag, y=t.value, colour=names), shape=21, fill='white') +
geom_hline(yintercept = h0.region[1], linetype='dashed') +
geom_hline(yintercept = h0.region[2], linetype='dashed') +
theme_bw() + guides(colour=FALSE)
saveRDS(df.se.results,
file='~/Google Drive/CMU/RIPS/paper/august17/resultsNW.rds')
}
}
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
fname <- paste0(path.out, 'plotTempLoad.png')
share.file.print.url(fname)
g
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
temp.breaks
coef(lm.model$model)
coef(lm.model$model)[1]
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
source('~/Google Drive/CMU/RIPS/git/demand/R/ripsSpecialPlots.R', echo=TRUE)
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
g
source('~/Google Drive/CMU/RIPS/git/demand/R/ripsSpecialPlots.R', echo=TRUE)
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
source('~/Google Drive/CMU/RIPS/git/demand/R/ripsSpecialPlots.R', echo=TRUE)
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
source('~/Google Drive/CMU/RIPS/git/demand/R/ripsSpecialPlots.R', echo=TRUE)
g <- plot.temp.load(reg.model=lm.model$model,
data.set=df.final,
temp.breaks = temp.breaks,
temp = c(-15:40), inter.term = NULL,
dir.out = path.out,
add.marg.hist = TRUE,
normalize.values = TRUE)
df.se.results <- readRDS(file='~/Google Drive/CMU/RIPS/paper/resultsNW.rds')
df.se.results
df.se.results %>% filter(lag == 14*24)
tabela <- df.se.results %>% filter(lag == 14*24)
tabela
round(tabela$Estimate,2)
round(tabela$Estimate,0)
round(tabela$std.error ,0)
round(tabela$std.error ,2)
round(tabela$std.error ,1)
# Chunk 1
library(ggplot2)
library(plyr)
library(dplyr)
library(tidyr)
#
levees.investment <- 10
storm.damage <- 20
prob.storm <- 1/50
disc.rate <- 0.03
n.years <- 50
n.sim <- 10000
#
pv.no.upgrade <- function(prob.storm=1/50, disc.rate=0.03, storm.damage=20,
n.years=50) {
# function to compute the NPV of the "NO UPGRADE" option. works for
# for both the expected value case and the simulation case
#
# present values of damages in each year
pvs <- -storm.damage*(1/(1+disc.rate))^c(1:n.years)
#
# prob storm can be a scalar or a vector of size n.years
pvs <- pvs * prob.storm
#
return(sum(pvs))
}
#
# part A: expected costs
#
exp.cost.upgrade <- -levees.investment
exp.cost.no.upgrade <- pv.no.upgrade()
# Chunk 2
# Part B: sensitivity analysis
discount.range <- seq(0.9*disc.rate, 1.1*disc.rate, length.out = 7)
NPV.discounts <- mapply(FUN=pv.no.upgrade, disc.rate=discount.range)
NPV.discounts <- data.frame(variable="discount rate",
delta=discount.range/disc.rate,
npv=NPV.discounts)
damage.range <- seq(0.9*storm.damage, 1.1*storm.damage, length.out = 7)
NPV.damage <- mapply(FUN=pv.no.upgrade, storm.damage=damage.range)
NPV.damage <- data.frame(variable="storm damage",
delta=damage.range/storm.damage,
npv=NPV.damage)
prob.range <- seq(0.9*prob.storm, 1.1*prob.storm, length.out = 7)
NPV.prob <- mapply(FUN=pv.no.upgrade, prob.storm=prob.range)
NPV.prob <- data.frame(variable="storm probability",
delta=prob.range/prob.storm,
npv=NPV.prob)
nyears.range <- seq(40, 60, length.out = 11)
NPV.nyears <- mapply(FUN=pv.no.upgrade, n.years=nyears.range)
NPV.nyears <- data.frame(variable="time horizon",
delta=nyears.range/n.years,
npv=NPV.nyears)
df.spider.plot <- rbind(NPV.discounts,NPV.damage, NPV.prob,NPV.nyears)
# Chunk 3
# set up colours for plot
colors <- c("red", "green", "blue", "brown", "orange", "purple")
# create spider plot
g <- ggplot() +
geom_line(data=df.spider.plot, aes(x=delta, y=npv, colour=variable),
size=1.1) +
theme_bw() + ylab("NPV (million dollars)") +
xlab('Percent change from baseline of input')
df.annotate <- df.spider.plot %>% group_by(variable) %>%
filter(delta == max(delta)) %>% as.data.frame()
# add layer of labels to our previous plot and remove legends
g <- g + geom_text(data=df.annotate, aes(x=delta+0.01, y=npv, label=variable),
hjust=0) + guides(colour=FALSE) + xlim(0.8,1.3)
g <- g + geom_hline(yintercept = exp.cost.upgrade, linetype='dashed') +
annotate('text', x=0.9, y=exp.cost.upgrade+0.05, label = "PV upgrade",
vjust=0, hjust=0)
print(g)
# Chunk 4
# two way sensitivity analysis
inv.range <- seq(5, 10, by=0.01)
damage.range <- seq(15, 25, by=0.01)
grid.sens.1 <- expand.grid(inv=inv.range,damage=damage.range)
grid.sens.1 <- data.frame(grid.sens.1,
npv.sens=mapply(FUN=pv.no.upgrade,
storm.damage=grid.sens.1$damage))
grid.sens.1$optimal.decision <- ifelse(-grid.sens.1$inv > grid.sens.1$npv.sens,
"UPGRADE", "NO UPGRADE")
g <- ggplot(grid.sens.1, aes(x=inv, y=damage)) +
geom_raster(aes(fill=optimal.decision)) +
scale_fill_manual(values = c("UPGRADE"='red','NO UPGRADE'='blue'))
print(g)
# Chunk 5
prob.range<- seq(0.005, 0.1, by=0.0001)
disc.range <- seq(0.02, 0.1, by=0.0001)
grid.sens.2 <- expand.grid(prob.storm=prob.range,disc.rate=disc.range)
grid.sens.2 <- data.frame(grid.sens.2,
npv.sens=mapply(FUN=pv.no.upgrade,
prob.storm=grid.sens.2$prob.storm,
disc.rate=grid.sens.2$disc.rate))
grid.sens.2$optimal.decision <- ifelse(-10 > grid.sens.2$npv.sens,
"UPGRADE", "NO UPGRADE")
g <- ggplot(grid.sens.2, aes(x=prob.storm, y=disc.rate)) +
geom_raster(aes(fill=optimal.decision)) +
scale_fill_manual(values = c("UPGRADE"='red','NO UPGRADE'='blue'))
print(g)
# Chunk 6
# Part C: montecarlo 1
set.seed(123)
results.mc1 <- rep(NA, n.sim)
for (i in 1:n.sim){
storm.event <- rbinom(n = n.years , size = 1, prob = prob.storm)
results.mc1[i] <- pv.no.upgrade(prob.storm = storm.event)
}
# compute some resulting statistics of our simulation
summary.stats.1 <- summary(results.mc1)
summary.stats.1 <- round(summary.stats.1, 2)
# Chunk 7
bins <- pretty(x=results.mc1, n=nclass.FD(results.mc1), min.n = 1)
bw <- bins[2] - bins[1]
g <- ggplot(data=data_frame(x=results.mc1)) +
geom_histogram(aes(x=x, y=..density..), fill='red', alpha=0.6,
binwidth = bw) +
theme_bw() + xlab('NPV ($ Billion)')
print(g)
# Chunk 8
# Part D: montecarlo 2
storm.year <- rep(NA, n.sim)
results.mc2 <- rep(NA, n.sim)
for (i in 1:n.sim){
storm.year[i] <- sample(n.years, 1)
storm.event <- rep(0, n.years)
storm.event[storm.year[i]] <- 1
results.mc2[i] <- pv.no.upgrade(prob.storm = storm.event)
}
# compute some resulting statistics of our simulation
summary.stats.2 <- summary(results.mc2)
summary.stats.2 <- round(summary.stats.2, 2)
# Chunk 9
# combine results from item (c) and (d) into one data frame
combined.df <- data.frame(item.c=results.mc1, item.d=results.mc2)
combined.df <- combined.df %>% gather(key='type', value='npv', item.c:item.d)
df.comparison <- combined.df %>% group_by(type) %>%
summarize(mean=mean(npv), quant5 = quantile(npv, 0.05),
quant95 = quantile(npv, 0.95))
g <- ggplot(data=combined.df) +
geom_histogram(aes(x=npv, y=..density..), fill='red', alpha=0.6,
binwidth = bw) +
facet_wrap(~type, nrow=2) + theme_bw() + xlab('NPV ($ Billion)')
print(g)
# Chunk 10
# Part E: montecarlo 3 (with utility function)
utility.function <- function(x, R=1) {
return(1-exp(-x/R))
}
inv.utility.function <- function(u, R=1) {
return(-R*log(1 - u))
}
util1 <- utility.function(x=results.mc1, R=1)
util2 <- utility.function(x=results.mc2, R=1)
CE_upgrade <- inv.utility.function(mean(-10))
CE1 <- inv.utility.function(mean(util1))
CE2 <- inv.utility.function(mean(util2))
sum(results.mc1 < -10)/n.sim
sum(results.mc1 < -20)/n.sim
sum(results.mc1 = 0)/n.sim
sum(results.mc1 == 0)/n.sim
results.mc1
sum(results.mc1 == 0)/n.sim
(1-0.02)^(50)
max(results.mc2)
min(results.mc2)
sum(results.mc1 < -10)/n.sim
sum(results.mc2 < -10)/n.sim
utility.function <- function(x, R=1) {
return(1-exp(-x/R))
}
inv.utility.function <- function(u, R=1) {
return(-R*log(1 - u))
}
certainty.equivalent <- function(x, R=1) {
exp.u <- mean(utility.function(x, R))
ce <- inv.utility.function(x, R)
return(ce)
}
CE_upgrade <- sapply(X=(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
CE_upgrade <- sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
CE_upgrade
CE_upgrade <- sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=10)
CE_upgrade
x = 10
mean(utility.function(x, R))
R = 1
mean(utility.function(x, R))
exp.u <- mean(utility.function(x, R))
certainty.equivalent <- function(x, R=1) {
exp.u <- mean(utility.function(x, R))
ce <- inv.utility.function(exp.u, R)
return(ce)
}
sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=10)
sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=10)
utility.function(x, R)
? mean
CE_upgrade <- sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
CE1 <- sapply(X=c(0.1, 1, 10, 100), certainty.equivalent, x=results.mc1)
CE2 <- sapply(X=c(0.1, 1, 10, 100), inv.utility.function, x=results.mc2)
CE_upgrade <- sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
CE1 <- sapply(X=c(0.1, 1, 10, 100), certainty.equivalent, x=results.mc1)
CE2 <- sapply(X=c(0.1, 1, 10, 100), inv.utility.function, x=results.mc2)
CE_upgrade <- sapply(X=c(0.1, 1, 10, 100), FUN=certainty.equivalent, x=-10)
CE1 <- sapply(X=c(0.1, 1, 10, 100), certainty.equivalent, x=results.mc1)
CE2 <- sapply(X=c(0.1, 1, 10, 100), certainty.equivalent, x=results.mc2)
df.annotate
df.annotate$npv[df.annotate == 'storm probability']
df.annotate$npv[df.annotate == 'storm probability'] + 0.5
df.annotate$npv[df.annotate == 'storm probability'] <- df.annotate$npv[df.annotate == 'storm probability'] + 0.5
df.annotate$npv[df.annotate == 'storm probability'] + 0.5
df.annotate$npv[df.annotate == 'storm probability']
df.annotate$npv[df.annotate$variable == 'storm probability'] <-
df.annotate$npv[df.annotate$variable == 'storm probability'] + 0.5
