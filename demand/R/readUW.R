#===================brief introduction=================#
#This file contains hourly meteorological data for the following two locations
#
#===================data information===================#
#Time period:   1950-01-01 00:00:00 to 2099-12-31 23:00:00
#Time step:     Hourly
#Column name:   
#[1] Precipitation (mm)
#[2] Specific humidity (kg/kg)
#[3] Relative humidity (fraction)
#[4] Air temperature (C)
#[5] Near surface atmosphere pressure (kPa)
#
#======================projection======================#
#Because the precision for meteorological data is 1/8 degree, the following
#specific location are projected into two different grid cells. The projected
#lat/lon represents the lat/lon for the center of the projected grid cells.
#
#
#- Nashville, TN (36.1627째 N, 86.7816째 W) projected lat/lon (36.1875, -86.8125)
#- Memphis, TN (35.1495째 N, 90.0490째 W)   projected lat/lon (35.1875, -90.0625)


library(RCurl)
library(XML)
library(lubridate)
library(plyr)
library(dplyr)

source("~/CMU/RIPS/git/demand/R/auxiliary_functions.R")

read.uw <- function(url.uw="~/GoogleDrive/CMU/RIPS/UW/meteo_memphis.txt",
                    years.data=c(2030, 2040)) {
  require(tools)
  
  if (file_ext(url.uw) == '.txt') {
    # txt
    uw.data <- read.uw.txt(url.uw = url.uw, years.data = years.data)
  } else {
    # rds
    uw.data <- readRDS(file=url.uw)
    names(uw.data) <- c("time", "temp", "Relative.humidity")
    uw.data$dp <- convertRelHum2DewPoint(uw.data$Relative.humidity,
                                         uw.data$temp)
    
    if (!is.null(years.data)) {
      uw.data <- uw.data %>% 
        dplyr::filter((year(time) >= years.data[1]) & 
                        (year(time) <= years.data[2]))
    }
    
    uw.data <- data.frame(year=year(uw.data$time), uw.data)
  }
  
  return(uw.data)
}

read.uw.txt <- function(url.uw="~/GoogleDrive/CMU/RIPS/UW/meteo_memphis.txt",
                        years.data=c(2030, 2040)) {
  # reads meteo data from txt file generated by UW
  #
  # years.data: vector with years being extracted
  
  names.cols.uw <- c("Precip", "Specific.humidity",  "Relative.humidity",
                     "temp", "pressure")
  
  uw.data <- read.table(file=url.uw, sep="\t", col.names = names.cols.uw)
  
  dates.uw <- seq(as.POSIXct("1950-01-01 00:00:00", tz="UTC"),
                  as.POSIXct("2099-12-31 23:00:00", tz="UTC"),
                  by="hour")
  
  uw.data <- data.frame(time=dates.uw, uw.data)
  
  uw.data.projection <- uw.data
  uw.data.projection$dp <- convertRelHum2DewPoint(uw.data.projection$Relative.humidity,
                                                  uw.data.projection$temp)
  
  if (!is.null(years.data)) {
    uw.data.projection <- uw.data.projection %>% 
      dplyr::filter((year(time) >= years.data[1]) & 
                      (year(time) <= years.data[2]))
  }
  
  uw.data.projection <- data.frame(year=year(uw.data.projection$time),
                                   uw.data.projection)
  return(uw.data.projection)
  
}

process.data <- function(df.data, per) {
  # auxiliary function that filters df.data and processes the data set
  # that will be used in the fucntion 'data.transformation'. It computes
  # 24 average hourly values for each month in the year (24 x 12 = 288 values).
  #
  # df.data: data frame with simulated data created by the climate model. The data
  #          frame must have a column "time" in a Date format and a 
  #          numeric column "value".
  # per    : vector with initial and final year of reference period
  #
  # Returns:
  # 

  require(plyr)
  require(dplyr)
  require(lubridate)
  
  # filter data inside range of years defined in 'per'
  data.out <- df.data %>% filter(year(time) <= per[2] & year(time) >= per[1])
  
  # create columns with month and hour to group and compute means
  data.out$month <- month(data.out$time)
  data.out$hour <- hour(data.out$time)
  
  xx <- data.out %>% group_by(month, hour) %>% 
    summarize(value=mean(value, na.rm=TRUE)) %>% arrange(month, hour) %>% 
    as.data.frame() 

  return(xx)
}

data.transformation <- function(data, base.line, ref.period=c(2005, 2015), 
                                future.period=c(2035, 2045)) {
  # this function applies a transformation to the projections of weather variables 
  # using the method suggested in http://www.nber.org/papers/w19087 
  # (section 4.3.2 'Correcting Aggregation Bias')
  #
  # The function computes the average change between the projections of a 
  # reference period and the projections in the future period. These changes 
  # are computed for each hour of the day for each month of the year 
  # (24 x 12 = 288 values)
  #
  # Then it adds this time series of deviations to a historical baseline of 
  # weather observations by matching the combination (month, hour).
  #
  # Reference, future data are all averaged to a single year. base.line is
  # kept in the original time span (averaging it was dampening the daily and 
  # intraday variability that is needed to estimate electricity demand)
  #
  # The idea is to perform the estimates of load in these transformed weather 
  # projections, since they would have reduced the bias from the climate models.
  #
  # Args:
  # data: data frame with simulated data created by the climate model. The data
  #       frame must have a column "time" in a Date format and a 
  #       numeric column "value".
  # baseline: data.frame with historical baseline data. Must contain the 
  #           exact period that will be used (no filtering will be made). 
  #           The data frame must have a column "time" in a Date format and a 
  #           numeric column "value".
  # ref.period: vector with initial and final year of reference period
  # future.period: vector with initial and final year of future period  
  
  require(plyr)
  require(dplyr)
  require(lubridate)

  # filter data to respective period and compute mean values
  xx <- process.data(df.data = data, per = ref.period)
  yy <- process.data(df.data = data, per = future.period)
  
  if (nrow(xx) != nrow(yy)) {
    stop("Length of computed reference data different from future data")
  }
  
  delta <- yy
  delta$value <- (yy$value - xx$value)

  data.out <- base.line
  
  # create columns with month and hour to group and compute means
  data.out$month <- month(data.out$time)
  data.out$hour <- hour(data.out$time)
  
  # join values of delta to proper (month, hour) rows
  # and sum up original values and deltas
  data.out <- data.out %>% 
    left_join(delta, by=c('month', 'hour')) %>% 
    transmute(time=time, value=value.x+value.y, delta=value.y)
  
  return(data.out)
  
}


data.transformation.paulina <- function(data, base.line, 
                                        ref.period=c(2005, 2015), 
                                        future.period=c(2035, 2045)) {
  # this function applies a transformation to the projections of weather variables 
  # using a variation of the method suggested in http://www.nber.org/papers/w19087 
  # (section 4.3.2 'Correcting Aggregation Bias'). This variation was proposed by
  # Paulina. Basically we compute the bias in the model data and subtract this
  # bias from the future projections. There is a evernote post that summarizes
  # the two methods.
  #
  # The function computes typical meteorological year in the historical data
  # and the typical meteorological year in the simulated data for the same period.
  # The it computes the average change between these two typical meteorological 
  # years (this is the model bias) . The change is then averaged 
  # for each hour of the day for each month of the year 
  # (24 x 12 = 288 values)
  #
  # Then it subtracts these deviations from the raw future projections 
  # (output from GCMs) of weather values by matching the combination (month, hour).
  #
  # The idea is to correct for possible bias in the GCM output, while trying
  # to preserve the variability in the future data set.
  #
  # Args:
  # data: data frame with simulated data created by the climate model. The data
  #       frame must have a column "time" in a Date format and a 
  #       numeric column "value".
  # baseline: data.frame with historical baseline data. Must contain the 
  #           exact period that will be used (no filtering will be made). 
  #           The data frame must have a column "time" in a Date format and a 
  #           numeric column "value".
  # ref.period: vector with initial and final year of reference period
  # future.period: vector with initial and final year of future period  
  
  require(plyr)
  require(dplyr)
  require(lubridate)
  
  # filter data to respective period and compute mean values
  xx <- process.data(df.data = base.line, per = ref.period) # typical year observed
  yy <- process.data(df.data = data, per = ref.period) # typical year simulated
  
  if (nrow(xx) != nrow(yy)) {
    stop("Length of computed reference data different from future data")
  }
  
  delta <- yy
  delta$value <- (yy$value - xx$value)

  data.out <- data %>% filter(year(time) <= future.period[2] & 
                                year(time) >= future.period[1])
  
  # create columns with month and hour to group and compute means
  data.out$month <- month(data.out$time)
  data.out$hour <- hour(data.out$time)
  
  # join values of delta to proper (month, hour) rows
  # and sum up original values and deltas
  data.out <- data.out %>% 
    left_join(delta, by=c('month', 'hour')) %>% 
    transmute(time=time, value=value.x - value.y, delta=value.y)
  
  return(data.out)
  
}


convert.txt2rds <- function(name.zip, yinf=-Inf, ysup=Inf, prefix=NULL, 
                            path.out='') {
  # this function reads a zip file that contains a set of txt files
  # with historical and projected values from several GCMs. It combines
  # historical and projected values into a single data.frame and 
  # saves the resulting data.frame in a RDS file
  
  cat('\n------- Processing file ', name.zip,  ' -----------\n', 
      sep='')
  
  # read list of files in zip
  list.files <- unzip(name.zip, list = TRUE)
  list.files <- list.files[list.files$Length > 0, 1]
  basedir <- unique(dirname(list.files))
  list.files <- basename(list.files)
  
  # pattern of names
  # historical files:
  # forcing_<MODEL>_historical_1950_2005.txt
  # projections:
  # forcing_<MODEL>_rcpXX_2006_2099.txt
  
  # get list of models
  name.models <- unique(sapply(X=list.files, 
                               FUN={function(x){strsplit(x, 
                                                         split = '_', 
                                                         fixed=TRUE)[[1]][2]}}, 
                               USE.NAMES = FALSE))
  for (mm in name.models){
    i <- which(name.models == mm)
    cat('\n------- Processing data GCM #', i, ' ', mm,  ' -----------\n', 
        sep='')
    
    ff.names <- grep(pattern = paste0('_', mm, '_'), list.files, 
                     value = TRUE, fixed = TRUE)
    
    # read historical data
    ff.hist <- grep(pattern = 'historical', ff.names, 
                    value = TRUE, fixed = TRUE)
    fout <- file.path(tempdir(), ff.hist)
    unzip(zipfile = name.zip, files = file.path(basedir, ff.hist), 
          exdir = tempdir(), junkpaths = TRUE)
    df.hist <- read.csv(file = fout)
    # convert to time
    df.hist[[1]] <- as.POSIXct(df.hist[[1]], tz='UTC')
    names(df.hist)[1] <- 'time'
    unlink(fout)
    
    # read RCP 4
    ff.rcp4 <- grep(pattern = 'rcp4', ff.names, 
                    value = TRUE, fixed = TRUE)
    fout <- file.path(tempdir(), ff.rcp4)
    unzip(zipfile = name.zip, files = file.path(basedir, ff.rcp4), 
          exdir = tempdir(), junkpaths = TRUE)
    df.rcp4 <- read.csv(file = fout)
    # convert to time
    df.rcp4[[1]] <- as.POSIXct(df.rcp4[[1]], tz='UTC')
    names(df.rcp4)[1] <- 'time'
    unlink(fout)
    
    # read RCP 8
    ff.rcp8 <- grep(pattern = 'rcp8', ff.names, 
                    value = TRUE, fixed = TRUE)
    fout <- file.path(tempdir(), ff.rcp8)
    unzip(zipfile = name.zip, files = file.path(basedir, ff.rcp8), 
          exdir = tempdir(), junkpaths = TRUE)
    df.rcp8 <- read.csv(file = fout)
    # convert to time
    df.rcp8[[1]] <- as.POSIXct(df.rcp8[[1]], tz='UTC')
    names(df.rcp8)[1] <- 'time'
    unlink(fout)
    
    # combine historical and projections
    df.rcp4 <- rbind(df.hist, df.rcp4)
    df.rcp8 <- rbind(df.hist, df.rcp8)
    rm(df.hist)
    
    df.rcp4 <- df.rcp4 %>% filter(year(time) >= yinf & year(time) <= ysup)
    df.rcp8 <- df.rcp8 %>% filter(year(time) >= yinf & year(time) <= ysup)
    
    # save as rds file
    saveRDS(df.rcp4, file=file.path(path.out, 
                                    paste0(ifelse(is.null(prefix), '', 
                                                  paste0(prefix,'_')), mm, 
                                           '_', 'rcp4.rds')))
    saveRDS(df.rcp8, file=file.path(path.out, 
                                    paste0(ifelse(is.null(prefix), '', 
                                                  paste0(prefix,'_')), mm, 
                                           '_', 'rcp8.rds')))
  }
}

read.data.meteo.uw <- function(nf.uw){
  # this function reads a csv/txt file that contains
  # past gridded weather data from the UofI METEO created by UW.
  # It saves the resulting data frame as a RDS file and silently returns
  # the data frame
  #
  # nf.uw: ocmplete path to csv/txt file
  # 
  # convention of columns:
  # [time, rel_humid, air_T]
  
  require(tools)
  
  df.uw <- read.csv(file=nf.uw, stringsAsFactors = FALSE)
  df.uw$X <- as.POSIXct(df.uw$X, tz='GMT')
  
  df.uw <- df.uw %>% 
    transmute(time=X, temp=air_T, 
              dp=convertRelHum2DewPoint(rel_humid, air_T)) %>%
    as.data.frame()
  
  path.uw <- dirname(nf.uw)
  nf.rds <- basename(file_path_sans_ext(nf.uw))
  
  saveRDS(df.uw, file=paste0(path.uw, '/', nf.rds, '.rds'))
  
  return(df.uw <- df.uw)
}


#if (FALSE) {
#  cat(paste0('\"/sharedstorage/user/fralston/GCM/', 
#             list.files(path='./out', pattern='rcp4'), '\", \n'), 
#      sep='          ', 
#      file='./out.txt')
#}

# compute average of days Feb-28 and Feb-29 for each year
# data.out.aux <- data.out %>% group_by(year(time), hour(time)) %>% 
#   filter(month(time) == 2 & (day(time) == 28 | day(time) == 29)) %>% 
#   summarise(time=time[1], value = mean(value, na.rm=TRUE)) %>% 
#   as.data.frame() %>% select(time, value) 

# remove Feb-29 from data set
# data.out <- data.out %>% filter(!(month(time) == 2 & day(time) == 29))

# replace data on Feb 28 with computed mean
# data.out$value[(month(data.out$time) == 2 & 
#                   day(data.out$time) == 28)] <- data.out.aux$value


#g <- ggplot() + geom_line(data=yy, aes(x=hour, y=delta)) + facet_wrap(~ month, nrow = 4, ncol=3)
#g <- g + geom_line(data=yy, aes(x=hour, y=value), linetype='dashed')
